{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import clean_data_loader as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imports import *\n",
    "import tensorflow_code as tc\n",
    "\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from tensorflow.contrib.rnn import GRUCell, LSTMCell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######\n",
      "Training Bidirectional / LSTMCell / Attention\n",
      "#######\n",
      "BIDRECTIONAL ENCODER\n",
      "ENCODER BASE CELL IS GRU\n",
      "DECODER BASE CELL GRU\n",
      "DECODER ATTENTOIN IS TRUE\n",
      "Epoch   1/100 Batch   20/1562 - Loss:  5.645, Seconds: 3.84\n",
      "Epoch   1/100 Batch   40/1562 - Loss:  3.274, Seconds: 3.44\n",
      "Epoch   1/100 Batch   60/1562 - Loss:  3.248, Seconds: 3.01\n",
      "Epoch   1/100 Batch   80/1562 - Loss:  3.155, Seconds: 2.79\n",
      "Epoch   1/100 Batch  100/1562 - Loss:  3.043, Seconds: 3.48\n",
      "Epoch   1/100 Batch  120/1562 - Loss:  3.100, Seconds: 3.06\n",
      "Epoch   1/100 Batch  140/1562 - Loss:  3.179, Seconds: 3.30\n",
      "Epoch   1/100 Batch  160/1562 - Loss:  3.148, Seconds: 2.92\n",
      "Epoch   1/100 Batch  180/1562 - Loss:  3.136, Seconds: 3.69\n",
      "Epoch   1/100 Batch  200/1562 - Loss:  3.026, Seconds: 3.06\n",
      "Epoch   1/100 Batch  220/1562 - Loss:  3.135, Seconds: 3.14\n",
      "Epoch   1/100 Batch  240/1562 - Loss:  3.146, Seconds: 2.89\n",
      "Epoch   1/100 Batch  260/1562 - Loss:  2.967, Seconds: 4.11\n",
      "Epoch   1/100 Batch  280/1562 - Loss:  2.933, Seconds: 3.40\n",
      "Epoch   1/100 Batch  300/1562 - Loss:  2.889, Seconds: 4.15\n",
      "Average loss for this update: 3.262 -- New Record!\n",
      "Epoch   1/100 Batch  320/1562 - Loss:  2.899, Seconds: 3.50\n",
      "Epoch   1/100 Batch  340/1562 - Loss:  2.971, Seconds: 3.27\n",
      "Epoch   1/100 Batch  360/1562 - Loss:  2.956, Seconds: 3.90\n",
      "Epoch   1/100 Batch  380/1562 - Loss:  2.904, Seconds: 2.98\n",
      "Epoch   1/100 Batch  400/1562 - Loss:  2.873, Seconds: 4.13\n",
      "Epoch   1/100 Batch  420/1562 - Loss:  2.848, Seconds: 3.56\n",
      "Epoch   1/100 Batch  440/1562 - Loss:  3.162, Seconds: 3.36\n",
      "Epoch   1/100 Batch  460/1562 - Loss:  2.811, Seconds: 3.09\n",
      "Epoch   1/100 Batch  480/1562 - Loss:  2.903, Seconds: 3.49\n",
      "Epoch   1/100 Batch  500/1562 - Loss:  2.999, Seconds: 3.79\n",
      "Epoch   1/100 Batch  520/1562 - Loss:  2.923, Seconds: 4.25\n",
      "Epoch   1/100 Batch  540/1562 - Loss:  2.953, Seconds: 3.15\n",
      "Epoch   1/100 Batch  560/1562 - Loss:  2.948, Seconds: 3.58\n",
      "Epoch   1/100 Batch  580/1562 - Loss:  2.617, Seconds: 3.56\n",
      "Epoch   1/100 Batch  600/1562 - Loss:  2.712, Seconds: 3.34\n",
      "Epoch   1/100 Batch  620/1562 - Loss:  2.730, Seconds: 3.78\n",
      "Average loss for this update: 2.877 -- New Record!\n",
      "Epoch   1/100 Batch  640/1562 - Loss:  2.791, Seconds: 3.35\n",
      "Epoch   1/100 Batch  660/1562 - Loss:  2.739, Seconds: 3.51\n",
      "Epoch   1/100 Batch  680/1562 - Loss:  2.675, Seconds: 4.15\n",
      "Epoch   1/100 Batch  700/1562 - Loss:  2.921, Seconds: 3.19\n",
      "Epoch   1/100 Batch  720/1562 - Loss:  3.105, Seconds: 3.99\n",
      "Epoch   1/100 Batch  740/1562 - Loss:  2.982, Seconds: 3.81\n",
      "Epoch   1/100 Batch  760/1562 - Loss:  2.959, Seconds: 2.90\n",
      "Epoch   1/100 Batch  780/1562 - Loss:  2.861, Seconds: 4.22\n",
      "Epoch   1/100 Batch  800/1562 - Loss:  2.728, Seconds: 3.38\n",
      "Epoch   1/100 Batch  820/1562 - Loss:  2.867, Seconds: 3.75\n",
      "Epoch   1/100 Batch  840/1562 - Loss:  2.626, Seconds: 3.60\n",
      "Epoch   1/100 Batch  860/1562 - Loss:  2.611, Seconds: 3.55\n",
      "Epoch   1/100 Batch  880/1562 - Loss:  2.415, Seconds: 3.16\n",
      "Epoch   1/100 Batch  900/1562 - Loss:  2.677, Seconds: 3.89\n",
      "Epoch   1/100 Batch  920/1562 - Loss:  2.647, Seconds: 3.35\n",
      "Average loss for this update: 2.77 -- New Record!\n",
      "Epoch   1/100 Batch  940/1562 - Loss:  2.614, Seconds: 3.19\n",
      "Epoch   1/100 Batch  960/1562 - Loss:  2.791, Seconds: 3.81\n",
      "Epoch   1/100 Batch  980/1562 - Loss:  2.894, Seconds: 4.04\n",
      "Epoch   1/100 Batch 1000/1562 - Loss:  2.884, Seconds: 3.66\n",
      "Epoch   1/100 Batch 1020/1562 - Loss:  2.744, Seconds: 3.23\n",
      "Epoch   1/100 Batch 1040/1562 - Loss:  2.874, Seconds: 3.65\n",
      "Epoch   1/100 Batch 1060/1562 - Loss:  2.747, Seconds: 3.42\n",
      "Epoch   1/100 Batch 1080/1562 - Loss:  2.751, Seconds: 4.29\n",
      "Epoch   1/100 Batch 1100/1562 - Loss:  2.751, Seconds: 3.23\n",
      "Epoch   1/100 Batch 1120/1562 - Loss:  2.624, Seconds: 3.66\n",
      "Epoch   1/100 Batch 1140/1562 - Loss:  2.604, Seconds: 3.65\n",
      "Epoch   1/100 Batch 1160/1562 - Loss:  2.705, Seconds: 3.43\n",
      "Epoch   1/100 Batch 1180/1562 - Loss:  2.553, Seconds: 3.02\n",
      "Epoch   1/100 Batch 1200/1562 - Loss:  2.760, Seconds: 3.99\n",
      "Epoch   1/100 Batch 1220/1562 - Loss:  2.667, Seconds: 3.47\n",
      "Epoch   1/100 Batch 1240/1562 - Loss:  2.576, Seconds: 3.68\n",
      "Average loss for this update: 2.722 -- New Record!\n",
      "Epoch   1/100 Batch 1260/1562 - Loss:  2.821, Seconds: 3.50\n",
      "Epoch   1/100 Batch 1280/1562 - Loss:  2.898, Seconds: 3.84\n",
      "Epoch   1/100 Batch 1300/1562 - Loss:  2.914, Seconds: 3.29\n",
      "Epoch   1/100 Batch 1320/1562 - Loss:  2.881, Seconds: 3.83\n",
      "Epoch   1/100 Batch 1340/1562 - Loss:  2.726, Seconds: 3.47\n",
      "Epoch   1/100 Batch 1360/1562 - Loss:  2.785, Seconds: 3.65\n",
      "Epoch   1/100 Batch 1380/1562 - Loss:  2.681, Seconds: 3.91\n",
      "Epoch   1/100 Batch 1400/1562 - Loss:  2.542, Seconds: 3.86\n",
      "Epoch   1/100 Batch 1420/1562 - Loss:  2.666, Seconds: 3.24\n",
      "Epoch   1/100 Batch 1440/1562 - Loss:  2.642, Seconds: 3.68\n",
      "Epoch   1/100 Batch 1460/1562 - Loss:  2.585, Seconds: 3.89\n",
      "Epoch   1/100 Batch 1480/1562 - Loss:  2.341, Seconds: 3.88\n",
      "Epoch   1/100 Batch 1500/1562 - Loss:  2.588, Seconds: 3.66\n",
      "Epoch   1/100 Batch 1520/1562 - Loss:  2.624, Seconds: 4.27\n",
      "Epoch   1/100 Batch 1540/1562 - Loss:  2.864, Seconds: 3.68\n",
      "Average loss for this update: 2.712 -- New Record!\n",
      "Epoch   1/100 Batch 1560/1562 - Loss:  2.765, Seconds: 3.95\n",
      "Epoch   2/100 Batch   20/1562 - Loss:  2.796, Seconds: 3.83\n",
      "Epoch   2/100 Batch   40/1562 - Loss:  2.395, Seconds: 3.47\n",
      "Epoch   2/100 Batch   60/1562 - Loss:  2.550, Seconds: 3.10\n",
      "Epoch   2/100 Batch   80/1562 - Loss:  2.517, Seconds: 2.90\n",
      "Epoch   2/100 Batch  100/1562 - Loss:  2.407, Seconds: 3.42\n",
      "Epoch   2/100 Batch  120/1562 - Loss:  2.482, Seconds: 3.12\n",
      "Epoch   2/100 Batch  140/1562 - Loss:  2.570, Seconds: 3.28\n",
      "Epoch   2/100 Batch  160/1562 - Loss:  2.510, Seconds: 2.86\n",
      "Epoch   2/100 Batch  180/1562 - Loss:  2.556, Seconds: 3.75\n",
      "Epoch   2/100 Batch  200/1562 - Loss:  2.438, Seconds: 3.10\n",
      "Epoch   2/100 Batch  220/1562 - Loss:  2.525, Seconds: 3.05\n",
      "Epoch   2/100 Batch  240/1562 - Loss:  2.556, Seconds: 2.86\n",
      "Epoch   2/100 Batch  260/1562 - Loss:  2.455, Seconds: 4.14\n",
      "Epoch   2/100 Batch  280/1562 - Loss:  2.372, Seconds: 3.26\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "start_time=time.time()\n",
    "\n",
    "# Hyperparameters\n",
    "#epochs = 10\n",
    "rnn_size = 256\n",
    "batch_size = 32\n",
    "num_layers = 2\n",
    "lr = 0.001\n",
    "keep_prob = 0.75\n",
    "\n",
    "GRU_BI_ATTN_CHECKPOINTDIR = './model_checkpoints/GRU_BI_ATTN/best_model.ckpt'\n",
    "GRU_BI_ATTN_LOSSES_PATH   = './checkpointed_data/losses/GRU_BI_ATTN_LOSS_ARR.p'\n",
    "\n",
    "''' \n",
    "    ENCODER STYLE:    BIDIRECTIONAL\n",
    "    LSTM CELL STYLE:  GRUCell\n",
    "    ATTENTION:        TRUE\n",
    "'''\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(1)\n",
    "tc.build_and_train_model(cl.word_embedding_matrix, \n",
    "                      rnn_size,\n",
    "                      num_layers,\n",
    "                      keep_prob,\n",
    "                      cl.vocab_to_int,\n",
    "                      batch_size,\n",
    "                      cl.sorted_summaries,\n",
    "                      cl.sorted_texts,\n",
    "                      encoder_style='bidirectional_rnn',\n",
    "                      attention=True,\n",
    "                      base_cell='GRU',\n",
    "                      checkpoint_file=GRU_BI_ATTN_CHECKPOINTDIR,\n",
    "                      losses_arr_path=GRU_BI_ATTN_LOSSES_PATH                      \n",
    "                     )\n",
    "\n",
    "end_time=time.time()\n",
    "print (\"Total time taken for training is =  \",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
